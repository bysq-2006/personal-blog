---
date: 2026-01-04
category:
  - 机器学习
---

# 反向传播具体过程
## 反向传播是什么？

得先说明，反向传播其实就是计算梯度下降算法中梯度的一个高效方法。

它和梯度下降实现的功能一模一样，都是为了让模型的损失函数值更小，从而提升模型的性能。

只不过反向传播的算法更高效，计算量更小。

举个例子的话，就像冒泡排序和快速排序，功能是一样的，都是为了把一堆数字排好序。只不过快速排序更高效，能更快地把数字排好序。

## 反向传播的工作原理

### 普通梯度下降与反向传播的区别

- **普通梯度下降**：直接对当前下降的参数求偏导，每次都要重复计算好多次重复的东西，导致计算量大且低效。
- **反向传播算法**：从最外层开始，一个一个的求要用的值，极大降低了计算量，提高了效率。

举个复杂的四层网络例子：假设有一个输入层（2个神经元）、两个隐藏层（隐藏层1有3个神经元，隐藏层2有2个神经元）和一个输出层（1个神经元）的神经网络。

### 前向传播算式

输入：$x = [x_1, x_2]^T$

**隐藏层1**：
$$
h_1^{(1)} = \sigma(w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1^{(1)})
$$
$$
h_2^{(1)} = \sigma(w_{21}^{(1)}x_1 + w_{22}^{(1)}x_2 + b_2^{(1)})
$$
$$
h_3^{(1)} = \sigma(w_{31}^{(1)}x_1 + w_{32}^{(1)}x_2 + b_3^{(1)})
$$

**隐藏层2**：
$$
h_1^{(2)} = \sigma(w_{11}^{(2)}h_1^{(1)} + w_{12}^{(2)}h_2^{(1)} + w_{13}^{(2)}h_3^{(1)} + b_1^{(2)})
$$
$$
h_2^{(2)} = \sigma(w_{21}^{(2)}h_1^{(1)} + w_{22}^{(2)}h_2^{(1)} + w_{23}^{(2)}h_3^{(1)} + b_2^{(2)})
$$

**输出层**：
$$
y = \sigma(v_1h_1^{(2)} + v_2h_2^{(2)} + b^{(3)})
$$

其中，$\sigma$ 是激活函数（例如 sigmoid 函数：$\sigma(z) = \frac{1}{1+e^{-z}}$），$w^{(l)}$ 表示第 $l$ 层的权重，$b^{(l)}$ 表示第 $l$ 层的偏置。

**完全展开的损失函数**（以简化形式展示）：

为了直观展示，我们将损失函数完全展开成关于输入 $x_1, x_2$ 的复合函数：

$$
L = \frac{1}{2} \left[ \sigma \left( v_1 \cdot \sigma \left( w_{11}^{(2)} \cdot \sigma(w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1^{(1)}) + w_{12}^{(2)} \cdot \sigma(w_{21}^{(1)}x_1 + w_{22}^{(1)}x_2 + b_2^{(1)}) + w_{13}^{(2)} \cdot \sigma(w_{31}^{(1)}x_1 + w_{32}^{(1)}x_2 + b_3^{(1)}) + b_1^{(2)} \right) + v_2 \cdot \sigma \left( w_{21}^{(2)} \cdot \sigma(w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1^{(1)}) + w_{22}^{(2)} \cdot \sigma(w_{21}^{(1)}x_1 + w_{22}^{(1)}x_2 + b_2^{(1)}) + w_{23}^{(2)} \cdot \sigma(w_{31}^{(1)}x_1 + w_{32}^{(1)}x_2 + b_3^{(1)}) + b_2^{(2)} \right) + b^{(3)} \right) - y_{target} \right]^2
$$

可以看到，这个表达式极其复杂，包含了多层嵌套的激活函数 $\sigma$ 和大量的权重参数。如果直接对每个参数求偏导，会产生大量的重复计算，这就是为什么需要反向传播算法的原因。

### 反向传播过程

反向传播从损失函数开始，逐步向内层拆解梯度。关键在于：每个中间值的梯度只需计算一次，然后被多个后续层复用，这正是反向传播高效的原因。

首先，从损失函数的最外层开始计算偏导：

$$
L = \frac{1}{2}(y - y_{target})^2
$$

**从最外层开始拆解**：

1. 对输出 $y$ 求偏导：
   $$
   \delta^{(3)} = \frac{\partial L}{\partial y} = y - y_{target}
   $$

2. 对输出层权重 $v_1$ 求偏导（使用链式法则）：
   $$
   \frac{\partial L}{\partial v_1} = \delta^{(3)} \cdot y(1-y) \cdot h_1^{(2)}
   $$

3. 对输出层权重 $v_2$ 求偏导：
   $$
   \frac{\partial L}{\partial v_2} = \delta^{(3)} \cdot y(1-y) \cdot h_2^{(2)}
   $$

4. 对输出层偏置 $b^{(3)}$ 求偏导：
   $$
   \frac{\partial L}{\partial b^{(3)}} = \delta^{(3)} \cdot y(1-y)
   $$

**注意**：这里计算了一次 $\delta^{(3)} \cdot y(1-y)$，之后的梯度计算都会复用这个值，避免重复计算。

5. 对隐藏层2的输出 $h_1^{(2)}$ 求偏导：
   $$
   \frac{\partial L}{\partial h_1^{(2)}} = \delta^{(3)} \cdot y(1-y) \cdot v_1
   $$

6. 对隐藏层2的输出 $h_2^{(2)}$ 求偏导：
   $$
   \frac{\partial L}{\partial h_2^{(2)}} = \delta^{(3)} \cdot y(1-y) \cdot v_2
   $$

7. 对隐藏层2的权重 $w_{11}^{(2)}$ 求偏导：
   $$
   \frac{\partial L}{\partial w_{11}^{(2)}} = \frac{\partial L}{\partial h_1^{(2)}} \cdot h_1^{(2)}(1-h_1^{(2)}) \cdot h_1^{(1)}
   $$

8. 对隐藏层2的权重 $w_{12}^{(2)}$ 求偏导：
   $$
   \frac{\partial L}{\partial w_{12}^{(2)}} = \frac{\partial L}{\partial h_1^{(2)}} \cdot h_1^{(2)}(1-h_1^{(2)}) \cdot h_2^{(1)}
   $$

9. 对隐藏层2的权重 $w_{13}^{(2)}$ 求偏导：
   $$
   \frac{\partial L}{\partial w_{13}^{(2)}} = \frac{\partial L}{\partial h_1^{(2)}} \cdot h_1^{(2)}(1-h_1^{(2)}) \cdot h_3^{(1)}
   $$

（隐藏层2其他权重的梯度类似...）

10. 对隐藏层1的输出 $h_1^{(1)}$ 求偏导：
    $$
    \frac{\partial L}{\partial h_1^{(1)}} = \frac{\partial L}{\partial h_1^{(2)}} \cdot h_1^{(2)}(1-h_1^{(2)}) \cdot w_{11}^{(2)} + \frac{\partial L}{\partial h_2^{(2)}} \cdot h_2^{(2)}(1-h_2^{(2)}) \cdot w_{21}^{(2)}
    $$

11. 对隐藏层1的输出 $h_2^{(1)}$ 求偏导：
    $$
    \frac{\partial L}{\partial h_2^{(1)}} = \frac{\partial L}{\partial h_1^{(2)}} \cdot h_1^{(2)}(1-h_1^{(2)}) \cdot w_{12}^{(2)} + \frac{\partial L}{\partial h_2^{(2)}} \cdot h_2^{(2)}(1-h_2^{(2)}) \cdot w_{22}^{(2)}
    $$

12. 对隐藏层1的输出 $h_3^{(1)}$ 求偏导：
    $$
    \frac{\partial L}{\partial h_3^{(1)}} = \frac{\partial L}{\partial h_1^{(2)}} \cdot h_1^{(2)}(1-h_1^{(2)}) \cdot w_{13}^{(2)} + \frac{\partial L}{\partial h_2^{(2)}} \cdot h_2^{(2)}(1-h_2^{(2)}) \cdot w_{23}^{(2)}
    $$

13. 对隐藏层1的权重 $w_{11}^{(1)}$ 求偏导：
    $$
    \frac{\partial L}{\partial w_{11}^{(1)}} = \frac{\partial L}{\partial h_1^{(1)}} \cdot h_1^{(1)}(1-h_1^{(1)}) \cdot x_1
    $$

14. 对隐藏层1的权重 $w_{12}^{(1)}$ 求偏导：
    $$
    \frac{\partial L}{\partial w_{12}^{(1)}} = \frac{\partial L}{\partial h_1^{(1)}} \cdot h_1^{(1)}(1-h_1^{(1)}) \cdot x_2
    $$

（隐藏层1其他权重和偏置的梯度类似...）

### 反向传播的计算高效性

观察上面的计算过程，我们可以看到：

- $\delta^{(3)} \cdot y(1-y)$ 这个值被计算一次，然后被用在多个后续的梯度计算中
- $\frac{\partial L}{\partial h_1^{(2)}} \cdot h_1^{(2)}(1-h_1^{(2)})$ 也被计算一次，然后被多次复用
- 每一层的梯度都基于上一层已经计算好的梯度，形成高效的链式传播

这样，反向传播通过**自下而上逐层计算梯度，并复用中间结果**，大幅降低了计算量。相比之下，普通梯度下降需要对每个参数从头独立计算梯度，会产生大量重复的子计算。

最后，使用这些梯度更新权重和偏置：
$$
w_{new} = w - \eta \frac{\partial L}{\partial w}, \quad b_{new} = b - \eta \frac{\partial L}{\partial b}
$$

其中 $\eta$ 是学习率。
