---
date: 2026-1-20
category:
  - 机器学习
title: 为什么分类任务我们要用交叉熵
---

## 前提
默认我们已经知道了普通的均方误差损失函数还有损失函数的作用。

## 交叉熵损失函数为什么存在？
大多数人第一次接触交叉熵函数应该都是在分类问题。

比如这里举一个经典的二分类任务，我们用一个模型来判断一个图片是不是猫。

假设如果是猫标签就为一，如果不是猫，标签就为零。

在这种情况下，输出一般是一个零到一之间的概率值，我们如果继续使用均方误差损失函数，可能会导致梯度消失或者收敛缓慢的问题。

而交叉熵损失函数正是为了解决这个问题而提出的。

我们可以假设一个模型：

## 结构假设

为了更好地理解交叉熵损失函数在分类任务中的应用，我们假设一个模型用于二分类任务（如判断图片是否为猫）。

### 网络结构概述
1. **模型本身（不包括输出层）**：一个简单的前馈神经网络，可以是多层感知机（MLP）或卷积神经网络（CNN），用于提取输入图片的特征。

2. **输出层**：
   - 这里有一个神经元，然后连接上一层的所有输出，不过在神经元的输出上会应用Sigmoid激活函数，将输出映射到0到1之间，表示属于正类的概率。

### Sigmoid函数详解
Sigmoid函数（也称为Logistic函数）是神经网络中常用的激活函数，其公式为：

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

这样来看，整个模型可以抽象为：

$$
\hat{y} = \frac{1}{1 + e^{-z}}
$$

y是模型输出的概率，也就是这一张图片为猫的概率。

z就是输出层里面连接的上一层所有输出的那一个神经元直接的输出。

大致图示如下： 

<div id="plot" style="width:100%;height:400px;"></div>

然后，我们定义损失函数，这里为了对比，我们先来看均方误差损失函数。

### 均方误差损失函数

均方误差（Mean Squared Error, MSE）损失函数是机器学习中最常用的损失函数之一，尤其适用于回归任务。它衡量预测值与真实值之间的平均平方差。

#### 公式
对于单个样本，MSE定义为(暂时只考虑单样本的情况)：

$$L(y, \hat{y}) = \frac{1}{2}(y - \hat{y})^2$$

完整的（图像假设y=1）：

$$L(y, \hat{y}) = \frac{1}{2}(y - \frac{1}{1 + e^{-z}})^2$$

<div id="loss-plot" style="width:100%;height:400px;"></div>
<div style="margin-top: 20px;">
  <label for="y-slider">y 真实值: <span id="y-value">1.00</span></label>
  <input id="y-slider" type="range" min="0" max="1" step="0.01" value="1" style="width: 100%; margin-top: 10px;">
</div>

**注意：这里为了举例，直接以最后一层输出层的输出作为x，实际上卷积神经网络有非常多的输入特征，是不可能直接可视化的。**

我们可以看到，当使用均方误差损失函数时，损失函数在接近0或1的预测值附近变化非常缓慢，这可能导致梯度消失，从而使模型训练变得困难。

### 交叉熵损失函数

二分类任务有一个特点，就是输出要么为零，要么为一。

而预测值也是零到一之间的，我想，要是有一个损失函数，能够在偏离真实值的时候，给出较大的惩罚，这样模型在训练过程中就能更快地收敛。

而距离真实值较近的时候又不至于误差太大一下子增加，从而避免梯度爆炸的问题。

符合这个特性的函数就是 **y = -log x**

但是很明显，这个函数只能处理一边的情况。

所以实际上我们可以分类讨论，分别考虑真实标签为1和0的情况。

#### 不同真实值的情况
- 当真实标签 $y=1$ 时，交叉熵损失函数为：
$$
L(1, \hat{y}) = -\log \hat{y}
$$
<div id="cross-entropy-1" style="width:100%;height:400px;"></div>

- 当真实标签 $y=0$ 时，交叉熵损失函数为：
$$
L(0, \hat{y}) = -\log (1-\hat{y})
$$
<div id="cross-entropy-0" style="width:100%;height:400px;"></div>

能够很直观的发现，当真实标签为1时，损失函数在预测值接近1时变化缓慢，而在预测值接近0时变化迅速；当真实标签为0时，损失函数在预测值接近0时变化缓慢，而在预测值接近1时变化迅速。

#### 把它们合起来
对于二分类任务，单个样本的交叉熵损失函数定义为：

$$
L(y, \hat{y}) = -\left[y \log \hat{y} + (1-y) \log (1-\hat{y})\right]
$$

其中：
- $y$ 是真实标签（0或1）
- $\hat{y}$ 是模型输出的概率（$\hat{y} = \sigma(z)$，即Sigmoid输出）

所以这就是为什么对于固定值的分类任务我们要用交叉熵而不是均方误差。

<script setup>
import { onMounted, ref, watch } from 'vue'
import Plotly from 'plotly.js/dist/plotly-basic.min.js'

const y_true = ref(1)

onMounted(() => {
	// 生成x和y数据
	const x = Array.from({length: 200}, (_, i) => -10 + i * 20 / 199)
	const y = x.map(v => 1 / (1 + Math.exp(-v)))
	Plotly.newPlot('plot', [{
		x, y,
		mode: 'lines',
		line: {color: '#0074D9'},
		name: 'Sigma',
	}], {
		margin: {l: 40, r: 10, t: 20, b: 40},
		xaxis: {title: 'x'},
		yaxis: {title: 'σ(x)', range: [0, 1]},
		showlegend: false,
	}, {displayModeBar: false})

	// 损失函数图像
	const updateLossPlot = () => {
		const loss_y = x.map(z => 0.5 * Math.pow(y_true.value - (1 / (1 + Math.exp(-z))), 2))
		Plotly.newPlot('loss-plot', [{
			x, y: loss_y,
			mode: 'lines',
			line: {color: '#FF4136'},
			name: 'Loss',
		}], {
			margin: {l: 40, r: 10, t: 20, b: 40},
			xaxis: {title: 'z', range: [-1, 1]},
			yaxis: {title: 'L(y, ŷ)'},
			showlegend: false,
		}, {displayModeBar: false})
	}

	updateLossPlot()

	// 绑定滑块事件
	const slider = document.getElementById('y-slider')
	const valueDisplay = document.getElementById('y-value')
	slider.addEventListener('input', (e) => {
		y_true.value = parseFloat(e.target.value)
		valueDisplay.textContent = y_true.value.toFixed(2)
		updateLossPlot()
	})

	// 交叉熵损失函数 y=1
	const x_ce = Array.from({length: 200}, (_, i) => 0.01 + i * 0.98 / 199)
	const y_ce1 = x_ce.map(v => -Math.log(v))
	Plotly.newPlot('cross-entropy-1', [{
		x: x_ce, y: y_ce1,
		mode: 'lines',
		line: {color: '#2ECC40'},
		name: 'Cross Entropy y=1',
	}], {
		margin: {l: 40, r: 10, t: 20, b: 40},
		xaxis: {title: 'ŷ'},
		yaxis: {title: 'L(1, ŷ)'},
		showlegend: false,
	}, {displayModeBar: false})

	// 交叉熵损失函数 y=0
	const y_ce0 = x_ce.map(v => -Math.log(1 - v))
	Plotly.newPlot('cross-entropy-0', [{
		x: x_ce, y: y_ce0,
		mode: 'lines',
		line: {color: '#FF851B'},
		name: 'Cross Entropy y=0',
	}], {
		margin: {l: 40, r: 10, t: 20, b: 40},
		xaxis: {title: 'ŷ'},
		yaxis: {title: 'L(0, ŷ)'},
		showlegend: false,
	}, {displayModeBar: false})
})
</script>

<style scoped>
div { flex: 1; max-width: 600px; }
</style>
